# Phase 1: API 테스트 결과

**날짜**: 2025-12-02
**모델**: llama2:7b-chat-q4_0 (4-bit 양자화)
**Docker Memory**: 12GB

## 테스트

**질문**: "What is 2 + 2? Answer briefly."
**답변**: "4"

## 성능 메트릭

- **총 소요 시간**: 6.18초
- **모델 로딩**: 3.37초 (54%)
- **프롬프트 처리**: 2.57초 (42%)
- **응답 생성**: 0.23초 (4%)
- **토큰 생성 속도**: ~13 tokens/sec

## 핵심 발견

### Cold Start 문제
- 첫 요청 시 모델 로딩에 3.4초 소요
- 전체 응답 시간의 54%
- **프로덕션 영향**: 첫 사용자는 6초 대기, 이후 사용자도 모델이 언로드되면 동일

### 메모리 문제 해결 과정
1. 초기 Docker 메모리: 5.2GB → 실패
2. 8GB로 증설 → 여전히 4.8GB 인식 → 실패
3. 컨테이너 재시작 후 → 성공

## 교훈

> LLM은 "즉시 응답"이 불가능하다. Cold start로 인한 초기 지연은 필연적.
