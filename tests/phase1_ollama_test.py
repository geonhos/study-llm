#!/usr/bin/env python3
"""
Ollama API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Ollama APIë¥¼ í†µí•´ LLMê³¼ ì§ˆì˜/ì‘ë‹µì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import requests
import json
import time

# Ollama API ì—”ë“œí¬ì¸íŠ¸
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "llama2:7b-chat-q4_0"


def check_ollama_status():
    """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags")
        if response.status_code == 200:
            print("âœ“ Ollama ì„œë²„ ì‹¤í–‰ ì¤‘")
            models = response.json().get("models", [])
            if models:
                print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models)}ê°œ")
                for model in models:
                    print(f"  - {model['name']} (í¬ê¸°: {model['size'] / (1024**3):.2f} GB)")
                return True
            else:
                print("âœ— ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì—†ìŒ")
                return False
        else:
            print(f"âœ— Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âœ— Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False


def generate_response(prompt, show_metrics=True):
    """
    Ollama APIë¡œ ì§ˆì˜í•˜ê³  ì‘ë‹µ ë°›ê¸°

    Args:
        prompt: ì§ˆì˜ ë‚´ìš©
        show_metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œì‹œ ì—¬ë¶€
    """
    url = f"{OLLAMA_URL}/api/generate"

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
    }

    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {prompt}")
    print(f"{'='*60}")

    start_time = time.time()

    try:
        response = requests.post(url, json=payload)

        if response.status_code == 200:
            result = response.json()
            elapsed_time = time.time() - start_time

            # ì‘ë‹µ ì¶œë ¥
            print(f"\në‹µë³€:\n{result['response']}")

            # ë©”íŠ¸ë¦­ ì¶œë ¥
            if show_metrics:
                print(f"\n{'â”€'*60}")
                print("ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
                print(f"  - ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                print(f"  - ìƒì„±ëœ í† í° ìˆ˜: {result.get('eval_count', 'N/A')}")
                print(f"  - í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {result.get('prompt_eval_count', 'N/A')}")

                if result.get('eval_count') and result.get('total_duration'):
                    tokens_per_sec = result['eval_count'] / (result['total_duration'] / 1e9)
                    print(f"  - í† í° ìƒì„± ì†ë„: {tokens_per_sec:.2f} tokens/sec")

                print(f"  - ëª¨ë¸ ë¡œë”© ì‹œê°„: {result.get('load_duration', 0) / 1e9:.2f}ì´ˆ")
                print(f"{'â”€'*60}")

            return result
        else:
            print(f"\nâœ— API ì˜¤ë¥˜: {response.status_code}")
            print(f"ë‚´ìš©: {response.text}")
            return None

    except Exception as e:
        print(f"\nâœ— ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return None


def chat_streaming(prompt):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸° (ì‹¤ì‹œê°„ ì¶œë ¥)

    Args:
        prompt: ì§ˆì˜ ë‚´ìš©
    """
    url = f"{OLLAMA_URL}/api/generate"

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    }

    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {prompt}")
    print(f"{'='*60}\n")
    print("ë‹µë³€: ", end="", flush=True)

    start_time = time.time()

    try:
        response = requests.post(url, json=payload, stream=True)

        if response.status_code == 200:
            full_response = ""

            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        text = chunk['response']
                        print(text, end="", flush=True)
                        full_response += text

                    if chunk.get('done', False):
                        elapsed_time = time.time() - start_time
                        print(f"\n\n{'â”€'*60}")
                        print(f"ğŸ“Š ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                        print(f"{'â”€'*60}")
                        break

            return full_response
        else:
            print(f"\nâœ— API ì˜¤ë¥˜: {response.status_code}")
            return None

    except Exception as e:
        print(f"\nâœ— ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return None


def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    print("\n" + "="*60)
    print("ëŒ€í™”í˜• ëª¨ë“œ (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥)")
    print("="*60)

    while True:
        try:
            prompt = input("\nì§ˆë¬¸: ").strip()

            if prompt.lower() in ['quit', 'exit', 'q']:
                print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not prompt:
                continue

            chat_streaming(prompt)

        except KeyboardInterrupt:
            print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


def run_test_queries():
    """í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰"""
    test_prompts = [
        "What is Python?",
        "Explain machine learning in one sentence.",
        "2 + 2ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
    ]

    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰")
    print("="*60)

    for prompt in test_prompts:
        result = generate_response(prompt, show_metrics=True)
        time.sleep(1)  # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸ¤– Ollama API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    print("="*60)

    # ì„œë²„ ìƒíƒœ í™•ì¸
    if not check_ollama_status():
        print("\nâš ï¸  ë¨¼ì € Ollama ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print("  docker-compose up -d")
        print("  docker exec ollama ollama pull llama2:7b-chat-q4_0")
        return

    # ë©”ë‰´ ì„ íƒ
    print("\nì„ íƒí•˜ì„¸ìš”:")
    print("  1. í…ŒìŠ¤íŠ¸ ì§ˆì˜ ì‹¤í–‰")
    print("  2. ëŒ€í™”í˜• ëª¨ë“œ")
    print("  3. ì¢…ë£Œ")

    choice = input("\nì„ íƒ (1-3): ").strip()

    if choice == "1":
        run_test_queries()
    elif choice == "2":
        interactive_mode()
    else:
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
